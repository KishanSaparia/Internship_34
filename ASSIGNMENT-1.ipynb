{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d12d19c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b10c77",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "Write a python program to display all the header tags from wikipedia.org."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3078e6b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H1 tags =  ['Main Page', 'Welcome to Wikipedia']\n",
      "\n",
      "\n",
      "H2 tags =  [\"From today's featured article\", 'Did you know\\xa0...', 'In the news', 'On this day', \"Today's featured picture\", 'Other areas of Wikipedia', \"Wikipedia's sister projects\", 'Wikipedia languages', 'Navigation menu']\n",
      "\n",
      "\n",
      "H3 tags =  ['Personal tools', 'Namespaces', 'Views', 'Navigation', 'Contribute', 'Tools', 'Print/export', 'In other projects', 'Languages']\n",
      "\n",
      "\n",
      "H4 tags =  []\n",
      "\n",
      "\n",
      "H5 tags =  []\n",
      "\n",
      "\n",
      "H6 tags =  []\n"
     ]
    }
   ],
   "source": [
    "# send to get request to the webpage server to get the sorce code of the page\n",
    "page=requests.get('https://en.wikipedia.org/wiki/Main_Page')\n",
    "# print(page)\n",
    "\n",
    "#page Content\n",
    "soup=BeautifulSoup(page.content)\n",
    "# print(soup)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# for H1 tag\n",
    "h1=soup.find('h1')\n",
    "# print(h1.text)\n",
    "\n",
    "#find all h1 tags\n",
    "h1=[]\n",
    "for i in soup.find_all('h1'):\n",
    "    h1.append(i.text)\n",
    "print('H1 tags = ',h1)\n",
    "print('\\n')\n",
    "\n",
    "#for H2 tag\n",
    "h2=soup.find('h2')\n",
    "# print(h2.text)\n",
    "\n",
    "# find all h2 tags\n",
    "h2=[]\n",
    "for i in soup.find_all('h2'):\n",
    "    h2.append(i.text)\n",
    "print('H2 tags = ',h2)\n",
    "print('\\n')\n",
    "\n",
    "# for h3 tag\n",
    "h3=soup.find('h3')\n",
    "# print(h3.text)\n",
    "\n",
    "# find all h3 tags\n",
    "h3=[]\n",
    "for i in soup.find_all('h3'):\n",
    "    i=i.text\n",
    "    h3.append(i.strip())\n",
    "print('H3 tags = ',h3)\n",
    "print('\\n')\n",
    "\n",
    "# for h4 tag\n",
    "h4=soup.find('h4')\n",
    "# print(h4)\n",
    "\n",
    "# find all h4 tags\n",
    "h4=[]\n",
    "for i in soup.find_all('h4'):\n",
    "    h4.append(i.text)\n",
    "print('H4 tags = ',h4)\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "# for h5 tag\n",
    "h5=soup.find('h5')\n",
    "# print(h5)\n",
    "\n",
    "# find all h5 tags\n",
    "h5=[]\n",
    "for i in soup.find_all('h5'):\n",
    "    h5.append(i.text)\n",
    "print('H5 tags = ',h5)\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "# for h6 tag\n",
    "h6=soup.find('h6')\n",
    "# print(h6)\n",
    "\n",
    "# find all h6 tags\n",
    "h6=[]\n",
    "for i in soup.find_all('h6'):\n",
    "    h6.append(i.text)\n",
    "print('H6 tags = ',h6)\n",
    "# AS we observe in last three tags h4,h5 and h6 are not available in website"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c10f5ee",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "Write a python program to display IMDB’s Top rated 100 movies’ data (i.e. name, rating, year of release)\n",
    "and make data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f272c8d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Movie</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Year of releace</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The Shawshank Redemption</td>\n",
       "      <td>9.3</td>\n",
       "      <td>1994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>The Godfather</td>\n",
       "      <td>9.2</td>\n",
       "      <td>1972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>The Dark Knight</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>The Lord of the Rings: The Return of the King</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Schindler's List</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>96</td>\n",
       "      <td>North by Northwest</td>\n",
       "      <td>8.3</td>\n",
       "      <td>1959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97</td>\n",
       "      <td>Vertigo</td>\n",
       "      <td>8.3</td>\n",
       "      <td>1958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98</td>\n",
       "      <td>Singin' in the Rain</td>\n",
       "      <td>8.3</td>\n",
       "      <td>1952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99</td>\n",
       "      <td>Citizen Kane</td>\n",
       "      <td>8.3</td>\n",
       "      <td>1941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>100</td>\n",
       "      <td>M - Eine Stadt sucht einen Mörder</td>\n",
       "      <td>8.3</td>\n",
       "      <td>1931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Rank                                          Movie  Rating  \\\n",
       "0      1                       The Shawshank Redemption     9.3   \n",
       "1      2                                  The Godfather     9.2   \n",
       "2      3                                The Dark Knight     9.0   \n",
       "3      4  The Lord of the Rings: The Return of the King     9.0   \n",
       "4      5                               Schindler's List     9.0   \n",
       "..   ...                                            ...     ...   \n",
       "95    96                             North by Northwest     8.3   \n",
       "96    97                                        Vertigo     8.3   \n",
       "97    98                            Singin' in the Rain     8.3   \n",
       "98    99                                   Citizen Kane     8.3   \n",
       "99   100              M - Eine Stadt sucht einen Mörder     8.3   \n",
       "\n",
       "    Year of releace  \n",
       "0              1994  \n",
       "1              1972  \n",
       "2              2008  \n",
       "3              2003  \n",
       "4              1993  \n",
       "..              ...  \n",
       "95             1959  \n",
       "96             1958  \n",
       "97             1952  \n",
       "98             1941  \n",
       "99             1931  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # send to get request to the webpage server to get the sorce code of the pages\n",
    "page1=requests.get(\"https://www.imdb.com/search/title/?groups=top_100&view=simple&sort=user_rating,desc&ref_=adv_prv\")\n",
    "page2=requests.get(\"https://www.imdb.com/search/title/?groups=top_100&view=simple&sort=user_rating,desc&start=51&ref_=adv_nxt\")\n",
    "# # print(page1)\n",
    "# # print(page2)\n",
    "\n",
    "# #page Content\n",
    "soup1=BeautifulSoup(page1.content)\n",
    "soup2=BeautifulSoup(page2.content)\n",
    "# # print(soup1)\n",
    "# # print(soup2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# rank for 100 movies\n",
    "\n",
    "# for first 50 movies\n",
    "\n",
    "# for first movie\n",
    "rank1=soup1.find('span',class_='lister-item-index unbold text-primary')\n",
    "# print(rank1.text)\n",
    "\n",
    "first50=[]\n",
    "for i in soup1.find_all('span',class_='lister-item-index unbold text-primary'):\n",
    "    i=i.text\n",
    "    i=int(re.sub(r'[^0-9]','',i))\n",
    "    first50.append(i)\n",
    "# print(first50)\n",
    "\n",
    "# for page 2 first movie\n",
    "rank2=soup2.find('span',class_='lister-item-index unbold text-primary')\n",
    "# print(rank2.text)\n",
    "\n",
    "page2_51to100=[]\n",
    "for i in soup2.find_all('span',class_='lister-item-index unbold text-primary'):\n",
    "    i=i.text\n",
    "    i=int(re.sub(r'[^0-9]','',i))\n",
    "    page2_51to100.append(i)\n",
    "# print(page2_51to100)\n",
    "\n",
    "rank=first50+page2_51to100\n",
    "# print('Rank=',rank)\n",
    "# print('\\n')\n",
    "\n",
    "# year of release 100 movies\n",
    "\n",
    "# for first 50 movies\n",
    "# for first movie\n",
    "year1=soup1.find('span',class_='lister-item-year text-muted unbold')\n",
    "# print(year1.text)\n",
    "\n",
    "year50=[]\n",
    "for i in soup1.find_all('span',class_='lister-item-year text-muted unbold'):\n",
    "    year50.append(i.text)\n",
    "# print(year50)\n",
    "\n",
    "# for page 2 first (51) movie release year\n",
    "year2=soup2.find('span',class_='lister-item-year text-muted unbold')\n",
    "# print(year2.text)\n",
    "\n",
    "page2_51to100=[]\n",
    "for i in soup2.find_all('span',class_='lister-item-year text-muted unbold'):\n",
    "    page2_51to100.append(i.text)\n",
    "# print(page2_51to100)\n",
    "\n",
    "year1=year50+page2_51to100\n",
    "# print(year1)\n",
    "\n",
    "year=[]\n",
    "for i in year1:\n",
    "    i=int(re.sub(r'[^0-9]','',i))\n",
    "    year.append(i)\n",
    "# print('year of release=',year)\n",
    "# print(\"\\n\")\n",
    "\n",
    "\n",
    "# rating of 100 movies\n",
    "\n",
    "# for first 50 movies rating\n",
    "# for first movie\n",
    "rating1=soup1.find('div',class_='col-imdb-rating')\n",
    "#print(rating1)\n",
    "\n",
    "rating50=[]\n",
    "for i in soup1.find_all('div',class_='col-imdb-rating'):\n",
    "    i=i.text.strip()\n",
    "    rating50.append(float(i))\n",
    "\n",
    "# for page 2 first (51) movie rating\n",
    "rating2=soup2.find('div',class_='col-imdb-rating')\n",
    "# print(rating2.text)\n",
    "\n",
    "page2_51to100=[]\n",
    "for i in soup2.find_all('div',class_='col-imdb-rating'):\n",
    "    i=i.text.strip()\n",
    "    page2_51to100.append(float(i))\n",
    "# print(page2_51to100)\n",
    "rating=rating50+page2_51to100\n",
    "# print('Rating=',rating)\n",
    "# print('\\n')\n",
    "\n",
    "# name of 100 movies\n",
    "\n",
    "# for first 50 name rating\n",
    "# for first movie\n",
    "\n",
    "name1=soup1.find('span',class_='lister-item-header')\n",
    "#print(name1)\n",
    "\n",
    "name50=[]\n",
    "for i in soup1.find_all('span',class_='lister-item-header'):\n",
    "    name50.append(i.a.text)\n",
    "\n",
    "# for page 2 first (51) movie rating\n",
    "name2=soup2.find('span',class_='lister-item-header')\n",
    "# print(name2.a.text)\n",
    "\n",
    "page2_51to100=[]\n",
    "for i in soup2.find_all('span',class_='lister-item-header'):\n",
    "    page2_51to100.append(i.a.text)\n",
    "# print(page2_51to100)\n",
    "\n",
    "name=name50+page2_51to100\n",
    "# print('Movie name=',name)\n",
    "# print('\\n')\n",
    "\n",
    "\n",
    "#creation of data Frame\n",
    "df=pd.DataFrame({'Rank':rank,'Movie':name,\"Rating\":rating,\"Year of releace\":year})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd2469f",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "Write a python program to display IMDB’s Top rated 100 Indian movies’ data (i.e. name, rating, year of\n",
    "release) and make data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90d4429a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Movie</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Year of releace</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Sagara Sangamam</td>\n",
       "      <td>8.8</td>\n",
       "      <td>1983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Goopy Gyne Bagha Byne</td>\n",
       "      <td>8.7</td>\n",
       "      <td>1969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Nayakan</td>\n",
       "      <td>8.6</td>\n",
       "      <td>1987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Pushpaka Vimana</td>\n",
       "      <td>8.6</td>\n",
       "      <td>1987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Apur Sansar</td>\n",
       "      <td>8.5</td>\n",
       "      <td>1959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>96</td>\n",
       "      <td>Jis Desh Men Ganga Behti Hai</td>\n",
       "      <td>7.2</td>\n",
       "      <td>1960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97</td>\n",
       "      <td>Vanaja</td>\n",
       "      <td>7.2</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98</td>\n",
       "      <td>Enthiran</td>\n",
       "      <td>7.1</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99</td>\n",
       "      <td>Chandni</td>\n",
       "      <td>6.7</td>\n",
       "      <td>1989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>100</td>\n",
       "      <td>Neecha Nagar</td>\n",
       "      <td>6.7</td>\n",
       "      <td>1946</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Rank                         Movie  Rating  Year of releace\n",
       "0      1               Sagara Sangamam     8.8             1983\n",
       "1      2         Goopy Gyne Bagha Byne     8.7             1969\n",
       "2      3                       Nayakan     8.6             1987\n",
       "3      4               Pushpaka Vimana     8.6             1987\n",
       "4      5                   Apur Sansar     8.5             1959\n",
       "..   ...                           ...     ...              ...\n",
       "95    96  Jis Desh Men Ganga Behti Hai     7.2             1960\n",
       "96    97                        Vanaja     7.2             2006\n",
       "97    98                      Enthiran     7.1             2010\n",
       "98    99                       Chandni     6.7             1989\n",
       "99   100                  Neecha Nagar     6.7             1946\n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # send to get request to the webpage server to get the sorce code of the pages\n",
    "page=requests.get(\"https://www.imdb.com/list/ls056092300/?sort=user_rating,desc&st_dt=&mode=detail&page=1\")\n",
    "# print(page)\n",
    "\n",
    "#page Content\n",
    "soup=BeautifulSoup(page.content)\n",
    "# print(soup)\n",
    "\n",
    "\n",
    "# rank for 100 movies\n",
    "\n",
    "# for first movie\n",
    "rank=soup.find('span',class_='lister-item-index unbold text-primary')\n",
    "#print(rank.text)\n",
    "\n",
    "first=[]\n",
    "for i in soup.find_all('span',class_='lister-item-index unbold text-primary'):\n",
    "    i=i.text\n",
    "    i=int(re.sub(r'[^0-9]','',i))\n",
    "    first.append(i)\n",
    "# print(first)\n",
    "\n",
    "\n",
    "rank=first\n",
    "# print('Rank=',rank)\n",
    "# print('\\n')\n",
    "\n",
    "\n",
    "# year of release 100 movies\n",
    "\n",
    "# for first movie\n",
    "year=soup.find('span',class_='lister-item-year text-muted unbold')\n",
    "# print(year.text)\n",
    "\n",
    "year=[]\n",
    "for i in soup.find_all('span',class_='lister-item-year text-muted unbold'):\n",
    "    year.append(i.text)\n",
    "# print(year)\n",
    "\n",
    "year1=[]\n",
    "for i in year:\n",
    "    i=int(re.sub(r'[^0-9]','',i))\n",
    "    year1.append(i)\n",
    "# print('year of release=',year1)\n",
    "# print(\"\\n\")\n",
    "\n",
    "\n",
    "# rating of 100 movies\n",
    "\n",
    "# for first movie\n",
    "rating1=soup.find('div',class_=\"ipl-rating-star small\")\n",
    "#print(rating1)\n",
    "\n",
    "rating=[]\n",
    "for i in soup.find_all('div',class_=\"ipl-rating-star small\"):\n",
    "    i=i.text.strip()\n",
    "    rating.append(float(i))\n",
    "\n",
    "# print('Rating=',rating)\n",
    "# print('\\n')\n",
    "\n",
    "# name of 100 movies\n",
    "# for first movie\n",
    "\n",
    "name=soup.find('h3',class_='lister-item-header')\n",
    "#print(name)\n",
    "\n",
    "name100=[]\n",
    "for i in soup.find_all('h3',class_='lister-item-header'):\n",
    "    name100.append(i.a.text)\n",
    "\n",
    "# print('Movie name=',name100)\n",
    "# print('\\n')\n",
    "\n",
    "df=pd.DataFrame({'Rank':rank,'Movie':name100,\"Rating\":rating,\"Year of releace\":year1})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b91042",
   "metadata": {},
   "source": [
    "# Question 4\n",
    "Write s python program to display list of respected former presidents of India(i.e. Name , Term of office)\n",
    "from https://presidentofindia.nic.in/former-presidents.htm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a083527e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name of President</th>\n",
       "      <th>Term of office</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Shri Ram Nath Kovind</td>\n",
       "      <td>25 July, 2017 to 25 July, 2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Shri Pranab Mukherj</td>\n",
       "      <td>25 July, 2012 to 25 July, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Smt Pratibha Devisingh Patil</td>\n",
       "      <td>25 July, 2007 to 25 July, 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DR. A.P.J. Abdul Kal</td>\n",
       "      <td>25 July, 2002 to 25 July, 2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Shri K. R. Narayanan</td>\n",
       "      <td>25 July, 1997 to 25 July, 2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Dr Shankar Dayal Shar</td>\n",
       "      <td>25 July, 1992 to 25 July, 1997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Shri R Venkataram</td>\n",
       "      <td>25 July, 1987 to 25 July, 1992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Giani Zail Sin</td>\n",
       "      <td>25 July, 1982 to 25 July, 1987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Shri Neelam Sanjiva Red</td>\n",
       "      <td>25 July, 1977 to 25 July, 1982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Dr. Fakhruddin Ali Ahm</td>\n",
       "      <td>24 August, 1974 to 11 February, 1977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Shri Varahagiri Venkata Gi</td>\n",
       "      <td>3 May, 1969 to 20 July, 1969 and 24 August, 19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Dr. Zakir Husa</td>\n",
       "      <td>13 May, 1967 to 3 May, 1969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Dr. Sarvepalli Radhakrishn</td>\n",
       "      <td>13 May, 1962 to 13 May, 1967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Dr. Rajendra Prasa</td>\n",
       "      <td>26 January, 1950 to 13 May, 1962</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Name of President  \\\n",
       "0           Shri Ram Nath Kovind    \n",
       "1             Shri Pranab Mukherj   \n",
       "2   Smt Pratibha Devisingh Patil    \n",
       "3            DR. A.P.J. Abdul Kal   \n",
       "4            Shri K. R. Narayanan   \n",
       "5           Dr Shankar Dayal Shar   \n",
       "6               Shri R Venkataram   \n",
       "7                  Giani Zail Sin   \n",
       "8         Shri Neelam Sanjiva Red   \n",
       "9          Dr. Fakhruddin Ali Ahm   \n",
       "10     Shri Varahagiri Venkata Gi   \n",
       "11                 Dr. Zakir Husa   \n",
       "12     Dr. Sarvepalli Radhakrishn   \n",
       "13             Dr. Rajendra Prasa   \n",
       "\n",
       "                                       Term of office  \n",
       "0                     25 July, 2017 to 25 July, 2022   \n",
       "1                     25 July, 2012 to 25 July, 2017   \n",
       "2                     25 July, 2007 to 25 July, 2012   \n",
       "3                     25 July, 2002 to 25 July, 2007   \n",
       "4                     25 July, 1997 to 25 July, 2002   \n",
       "5                     25 July, 1992 to 25 July, 1997   \n",
       "6                     25 July, 1987 to 25 July, 1992   \n",
       "7                     25 July, 1982 to 25 July, 1987   \n",
       "8                     25 July, 1977 to 25 July, 1982   \n",
       "9                24 August, 1974 to 11 February, 1977  \n",
       "10  3 May, 1969 to 20 July, 1969 and 24 August, 19...  \n",
       "11                        13 May, 1967 to 3 May, 1969  \n",
       "12                       13 May, 1962 to 13 May, 1967  \n",
       "13                   26 January, 1950 to 13 May, 1962  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # send to get request to the webpage server to get the sorce code of the pages\n",
    "page=requests.get(\"https://presidentofindia.nic.in/former-presidents.htm\")\n",
    "# print(page1)\n",
    "\n",
    "# #page Content\n",
    "soup=BeautifulSoup(page.content)\n",
    "# print(soup)\n",
    "\n",
    "\n",
    "\n",
    "#data for president birth,name,term of office\n",
    "\n",
    "first_term=soup.find('div',class_=\"presidentListing\")\n",
    "# for find name \n",
    "# print(first_term.h3.text)\n",
    "name=first_term.h3.text\n",
    "name_size=slice(0,-14)\n",
    "# print(name[name_size])\n",
    "\n",
    "# for find term of office\n",
    "term=first_term.p.text\n",
    "term_size=slice(16,-1)\n",
    "# print(term[term_size])\n",
    "\n",
    "#fetch all name and term of office data\n",
    "name=[]\n",
    "term=[]\n",
    "name_size=slice(0,-14)\n",
    "term_size=slice(16,-1,)\n",
    "for i in soup.find_all('div',class_=\"presidentListing\"):\n",
    "    j=i.p.text\n",
    "    i=i.h3.text\n",
    "    name.append(i[name_size])\n",
    "    term.append(j[16:])\n",
    "# print(\"Name =\",name)\n",
    "# print('\\n')\n",
    "# print('Term of office =',term)\n",
    "\n",
    "df=pd.DataFrame({\"Name of President\":name,\n",
    "                \"Term of office\":term})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947bc4c6",
   "metadata": {},
   "source": [
    "# Question 5\n",
    "Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9ffe9757",
   "metadata": {},
   "source": [
    "a) Top 10 ODI teams in men’s cricket along with the records for matches, points and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58c80f8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Team Name</th>\n",
       "      <th>Matches</th>\n",
       "      <th>Points</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>New Zealand</td>\n",
       "      <td>23</td>\n",
       "      <td>2,670</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>England</td>\n",
       "      <td>30</td>\n",
       "      <td>3,400</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Australia</td>\n",
       "      <td>32</td>\n",
       "      <td>3,572</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>India</td>\n",
       "      <td>35</td>\n",
       "      <td>3,866</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pakistan</td>\n",
       "      <td>22</td>\n",
       "      <td>2,354</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>South Africa</td>\n",
       "      <td>24</td>\n",
       "      <td>2,392</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Bangladesh</td>\n",
       "      <td>30</td>\n",
       "      <td>2,753</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sri Lanka</td>\n",
       "      <td>30</td>\n",
       "      <td>2,677</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>19</td>\n",
       "      <td>1,380</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>West Indies</td>\n",
       "      <td>41</td>\n",
       "      <td>2,902</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Team Name Matches Points Rating\n",
       "0   New Zealand      23  2,670    116\n",
       "1       England      30  3,400    113\n",
       "2     Australia      32  3,572    112\n",
       "3         India      35  3,866    110\n",
       "4      Pakistan      22  2,354    107\n",
       "5  South Africa      24  2,392    100\n",
       "6    Bangladesh      30  2,753     92\n",
       "7     Sri Lanka      30  2,677     89\n",
       "8   Afghanistan      19  1,380     73\n",
       "9   West Indies      41  2,902     71"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# send to get request to the webpage server to get the sorce code of the pages\n",
    "page=requests.get('https://www.icc-cricket.com/rankings/mens/team-rankings/odi')\n",
    "\n",
    "#page Content\n",
    "soup=BeautifulSoup(page.content)\n",
    "# print(soup)\n",
    "\n",
    "\n",
    "#team name\n",
    "team_1=soup.find('span',class_='u-hide-phablet')\n",
    "team_1.text\n",
    "#fetch all data for team name\n",
    "team=[]\n",
    "j=0\n",
    "for i in soup.find_all('span',class_='u-hide-phablet'):\n",
    "    team.append(i.text)\n",
    "    j=j+1\n",
    "    if(j==10):\n",
    "        break\n",
    "# print('Team Name =',team)\n",
    "# print('\\n')\n",
    "\n",
    "#Team matches\n",
    "team_1match=soup.find('td',class_='rankings-block__banner--matches')\n",
    "# print(team_1match.text)\n",
    "team_1match=list(team_1match)\n",
    "#Team matches\n",
    "team_1Points=soup.find('td',class_='rankings-block__banner--points')\n",
    "# print(team_1Points.text)\n",
    "team_1Points=list(team_1Points)\n",
    "#fetch all data for team matches\n",
    "match=[]\n",
    "for i in soup.find_all('td',class_='table-body__cell u-center-text'):\n",
    "    match.append(i.text)\n",
    "# print('Team Match =',match)\n",
    "\n",
    "#seprate team match and it's points\n",
    "totalmatches=[]\n",
    "points=[]\n",
    "\n",
    "for i in range(0,18):\n",
    "    if(i%2==0):\n",
    "        totalmatches.append(match[i])\n",
    "    else:\n",
    "        points.append(match[i])\n",
    "        \n",
    "matches=team_1match+totalmatches\n",
    "points=team_1Points+points\n",
    "# print(matches)\n",
    "# print('\\n')\n",
    "# print(points)\n",
    "# print('\\n')\n",
    "\n",
    "\n",
    "team_1rating=soup.find('td',class_='rankings-block__banner--rating u-text-right')\n",
    "team1_rating=team_1rating.text.split()\n",
    "# print(team_1rating.text.split())\n",
    "\n",
    "rating=[]\n",
    "j=0\n",
    "for i in soup.find_all('td',class_='table-body__cell u-text-right rating'):\n",
    "    rating.append(i.text)\n",
    "    j=j+1\n",
    "    if(j==9):\n",
    "        break\n",
    "# print('Team rating =',rating)\n",
    "rating=team1_rating+rating\n",
    "# print('Team rating =',rating)\n",
    "\n",
    "\n",
    "df=pd.DataFrame({\"Team Name\":team,\n",
    "                \"Matches\":matches,\n",
    "                \"Points\":points,\n",
    "                \"Rating\":rating})\n",
    "df"
   ]
  },
  {
   "cell_type": "raw",
   "id": "12a7da2b",
   "metadata": {},
   "source": [
    "b) Top 10 ODI Batsmen along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aed66523",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Country</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Babar Azam</td>\n",
       "      <td>PAK</td>\n",
       "      <td>890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Imam-ul-Haq</td>\n",
       "      <td>PAK</td>\n",
       "      <td>779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rassie van der Dussen</td>\n",
       "      <td>SA</td>\n",
       "      <td>766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Quinton de Kock</td>\n",
       "      <td>SA</td>\n",
       "      <td>759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>David Warner</td>\n",
       "      <td>AUS</td>\n",
       "      <td>747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Steve Smith</td>\n",
       "      <td>AUS</td>\n",
       "      <td>719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Jonny Bairstow</td>\n",
       "      <td>ENG</td>\n",
       "      <td>710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Virat Kohli</td>\n",
       "      <td>IND</td>\n",
       "      <td>707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Rohit Sharma</td>\n",
       "      <td>IND</td>\n",
       "      <td>704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Kane Williamson</td>\n",
       "      <td>NZ</td>\n",
       "      <td>701</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Name Country Rating\n",
       "0             Babar Azam     PAK    890\n",
       "1            Imam-ul-Haq     PAK    779\n",
       "2  Rassie van der Dussen      SA    766\n",
       "3        Quinton de Kock      SA    759\n",
       "4           David Warner     AUS    747\n",
       "5            Steve Smith     AUS    719\n",
       "6         Jonny Bairstow     ENG    710\n",
       "7            Virat Kohli     IND    707\n",
       "8           Rohit Sharma     IND    704\n",
       "9        Kane Williamson      NZ    701"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# send to get request to the webpage server to get the sorce code of the pages\n",
    "page=requests.get('https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting')\n",
    "\n",
    "#page Content\n",
    "soup=BeautifulSoup(page.content)\n",
    "# print(soup)\n",
    "\n",
    "\n",
    "\n",
    "#name of batsman\n",
    "#firstname\n",
    "fn=soup.find('div',class_='rankings-block__banner--name-large')\n",
    "fn=[str(fn.text)]\n",
    "# print(fn)\n",
    "\n",
    "#fetch all data for name\n",
    "name=[]\n",
    "j=0\n",
    "for i in soup.find_all('td',class_='table-body__cell rankings-table__name name'):\n",
    "    name.append(i.a.text)\n",
    "    j+=1\n",
    "    if (j==9):\n",
    "        break\n",
    "name=fn+name\n",
    "# print(\"Name of batsman = \",name)\n",
    "# print('\\n')\n",
    "\n",
    "#name of country\n",
    "#first name of country\n",
    "fc=soup.find('div',class_='rankings-block__banner--nationality')\n",
    "fc=fc.text.split()\n",
    "# print(fc)\n",
    "\n",
    "\n",
    "#fetch all data for country\n",
    "country=[]\n",
    "j=0\n",
    "for i in soup.find_all('span',class_='table-body__logo-text'):\n",
    "    country.append(i.text)\n",
    "    j+=1\n",
    "    if (j==9):\n",
    "        break\n",
    "country=fc+country\n",
    "# print('Country = ',country)\n",
    "# print('\\n')\n",
    "\n",
    "\n",
    "# rating of batsman\n",
    "#first rating\n",
    "fr=soup.find('div',class_='rankings-block__banner--rating')\n",
    "fr=[str(fr.text)]\n",
    "# print(fr)\n",
    "\n",
    "rating=[]\n",
    "j=0\n",
    "for i in soup.find_all('td',class_='table-body__cell rating'):\n",
    "    rating.append(i.text)\n",
    "    j+=1\n",
    "    if (j==9):\n",
    "        break\n",
    "rating=fr+rating\n",
    "# print('Rating = ',rating)\n",
    "# print('\\n')\n",
    "\n",
    "df=pd.DataFrame({\"Name\":name,\n",
    "                \"Country\":country,\n",
    "                \"Rating\":rating})\n",
    "df"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8c858b3d",
   "metadata": {},
   "source": [
    "c) Top 10 ODI bowlers along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d916fdf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Country</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Trent Boult</td>\n",
       "      <td>NZ</td>\n",
       "      <td>760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Josh Hazlewood</td>\n",
       "      <td>AUS</td>\n",
       "      <td>727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mitchell Starc</td>\n",
       "      <td>AUS</td>\n",
       "      <td>665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Shaheen Afridi</td>\n",
       "      <td>PAK</td>\n",
       "      <td>661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Matt Henry</td>\n",
       "      <td>NZ</td>\n",
       "      <td>656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Adam Zampa</td>\n",
       "      <td>AUS</td>\n",
       "      <td>655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Mehedi Hasan</td>\n",
       "      <td>BAN</td>\n",
       "      <td>655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Mujeeb Ur Rahman</td>\n",
       "      <td>AFG</td>\n",
       "      <td>650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Mustafizur Rahman</td>\n",
       "      <td>BAN</td>\n",
       "      <td>640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Rashid Khan</td>\n",
       "      <td>AFG</td>\n",
       "      <td>635</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Name Country Rating\n",
       "0        Trent Boult      NZ    760\n",
       "1     Josh Hazlewood     AUS    727\n",
       "2     Mitchell Starc     AUS    665\n",
       "3     Shaheen Afridi     PAK    661\n",
       "4         Matt Henry      NZ    656\n",
       "5         Adam Zampa     AUS    655\n",
       "6       Mehedi Hasan     BAN    655\n",
       "7   Mujeeb Ur Rahman     AFG    650\n",
       "8  Mustafizur Rahman     BAN    640\n",
       "9        Rashid Khan     AFG    635"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# send to get request to the webpage server to get the sorce code of the pages\n",
    "page=requests.get('https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling')\n",
    "\n",
    "#page Content\n",
    "soup=BeautifulSoup(page.content)\n",
    "# print(soup)\n",
    "\n",
    "\n",
    "#name of bowler\n",
    "#firstname\n",
    "fn=soup.find('div',class_='rankings-block__banner--name-large')\n",
    "fn=[str(fn.text)]\n",
    "# print(fn)\n",
    "\n",
    "#fetch all data for name\n",
    "name1=[]\n",
    "j=0\n",
    "for i in soup.find_all('td',class_='table-body__cell rankings-table__name name'):\n",
    "    name1.append(i.a.text)\n",
    "    j+=1\n",
    "    if (j==9):\n",
    "        break\n",
    "name=fn+name1\n",
    "# print(\"Name of bowler = \",name)\n",
    "# print('\\n')\n",
    "\n",
    "#name of country\n",
    "#first name of country\n",
    "fc=soup.find('div',class_='rankings-block__banner--nationality')\n",
    "fc=fc.text.split()\n",
    "# print(fc)\n",
    "\n",
    "\n",
    "#fetch all data for country\n",
    "country1=[]\n",
    "j=0\n",
    "for i in soup.find_all('span',class_='table-body__logo-text'):\n",
    "    country1.append(i.text)\n",
    "    j+=1\n",
    "    if (j==9):\n",
    "        break\n",
    "country=fc+country1\n",
    "# print('Country = ',country)\n",
    "# print('\\n')\n",
    "# rating of bowler\n",
    "#first rating\n",
    "fr=soup.find('div',class_='rankings-block__banner--rating')\n",
    "fr=[str(fr.text)]\n",
    "# print(fr)\n",
    "\n",
    "rating2=[]\n",
    "j=0\n",
    "for i in soup.find_all('td',class_='table-body__cell rating'):\n",
    "    rating2.append(i.text)\n",
    "    j+=1\n",
    "    if (j==9):\n",
    "        break\n",
    "rating=fr+rating2\n",
    "# print('Rating = ',rating)\n",
    "# print('\\n')\n",
    "\n",
    "df=pd.DataFrame({\"Name\":name,\n",
    "                \"Country\":country,\n",
    "                \"Rating\":rating})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8573aa5",
   "metadata": {},
   "source": [
    "# Question 6\n",
    "Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "827afe84",
   "metadata": {},
   "source": [
    "a) Top 10 ODI teams in women’s cricket along with the records for matches, points and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83e5691d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Team Name</th>\n",
       "      <th>Matches</th>\n",
       "      <th>Points</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Australia</td>\n",
       "      <td>18</td>\n",
       "      <td>3,061</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>South Africa</td>\n",
       "      <td>26</td>\n",
       "      <td>3,098</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>England</td>\n",
       "      <td>25</td>\n",
       "      <td>2,904</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>India</td>\n",
       "      <td>27</td>\n",
       "      <td>2,820</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>New Zealand</td>\n",
       "      <td>24</td>\n",
       "      <td>2,425</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>West Indies</td>\n",
       "      <td>24</td>\n",
       "      <td>2,334</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Bangladesh</td>\n",
       "      <td>12</td>\n",
       "      <td>932</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Thailand</td>\n",
       "      <td>8</td>\n",
       "      <td>572</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Pakistan</td>\n",
       "      <td>24</td>\n",
       "      <td>1,519</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sri Lanka</td>\n",
       "      <td>8</td>\n",
       "      <td>353</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Team Name Matches Points Rating\n",
       "0     Australia      18  3,061    170\n",
       "1  South Africa      26  3,098    119\n",
       "2       England      25  2,904    116\n",
       "3         India      27  2,820    104\n",
       "4   New Zealand      24  2,425    101\n",
       "5   West Indies      24  2,334     97\n",
       "6    Bangladesh      12    932     78\n",
       "7      Thailand       8    572     72\n",
       "8      Pakistan      24  1,519     63\n",
       "9     Sri Lanka       8    353     44"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# send to get request to the webpage server to get the sorce code of the pages\n",
    "page=requests.get('https://www.icc-cricket.com/rankings/womens/team-rankings/odi')\n",
    "\n",
    "#page Content\n",
    "soup=BeautifulSoup(page.content)\n",
    "# print(soup)\n",
    "\n",
    "\n",
    "\n",
    "#Women team name\n",
    "team=soup.find('span',class_='u-hide-phablet')\n",
    "team.text\n",
    "#fetch all data for team name\n",
    "teamf=[]\n",
    "j=0\n",
    "for i in soup.find_all('span',class_='u-hide-phablet'):\n",
    "    teamf.append(i.text)\n",
    "    j=j+1\n",
    "    if(j==10):\n",
    "        break\n",
    "# print('Women Team Name =',teamf)\n",
    "# print('\\n')\n",
    "\n",
    "#Team matches\n",
    "team_1match=soup.find('td',class_='rankings-block__banner--matches')\n",
    "# print(team_1match.text)\n",
    "team_1=list(team_1match)\n",
    "#Team matches\n",
    "team_1Points=soup.find('td',class_='rankings-block__banner--points')\n",
    "# print(team_1Points.text)\n",
    "team_1P=list(team_1Points)\n",
    "#fetch all data for team matches\n",
    "match=[]\n",
    "for i in soup.find_all('td',class_='table-body__cell u-center-text'):\n",
    "    match.append(i.text)\n",
    "# print('Team Match =',match)\n",
    "\n",
    "#seprate team match and it's points\n",
    "totalmatches=[]\n",
    "points=[]\n",
    "\n",
    "for i in range(0,18):\n",
    "    if(i%2==0):\n",
    "        totalmatches.append(match[i])\n",
    "    else:\n",
    "        points.append(match[i])\n",
    "        \n",
    "match=team_1+totalmatches\n",
    "point=team_1P+points\n",
    "# print('Women Team match =',match)\n",
    "# print('\\n')\n",
    "# print(\"Women Team point = \",point)\n",
    "# print('\\n')\n",
    "\n",
    "\n",
    "team_1rat=soup.find('td',class_='rankings-block__banner--rating u-text-right')\n",
    "team1_rat=team_1rat.text.split()\n",
    "# print(team_1rat.text.split())\n",
    "\n",
    "ratings=[]\n",
    "j=0\n",
    "for i in soup.find_all('td',class_='table-body__cell u-text-right rating'):\n",
    "    ratings.append(i.text)\n",
    "    j=j+1\n",
    "    if(j==9):\n",
    "        break\n",
    "# print('Team rating =',ratings)\n",
    "rating=team1_rat+ratings\n",
    "# print('Team rating =',rating)\n",
    "\n",
    "\n",
    "df=pd.DataFrame({\"Team Name\":teamf,\n",
    "                \"Matches\":match,\n",
    "                \"Points\":point,\n",
    "                \"Rating\":rating})\n",
    "df"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4b02c21b",
   "metadata": {},
   "source": [
    "b) Top 10 women’s ODI Batting players along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db42df4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Country</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alyssa Healy</td>\n",
       "      <td>NZ</td>\n",
       "      <td>760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Beth Mooney</td>\n",
       "      <td>AUS</td>\n",
       "      <td>749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Laura Wolvaardt</td>\n",
       "      <td>SA</td>\n",
       "      <td>732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Natalie Sciver</td>\n",
       "      <td>ENG</td>\n",
       "      <td>725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Harmanpreet Kaur</td>\n",
       "      <td>IND</td>\n",
       "      <td>716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Smriti Mandhana</td>\n",
       "      <td>IND</td>\n",
       "      <td>714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Meg Lanning</td>\n",
       "      <td>AUS</td>\n",
       "      <td>710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Rachael Haynes</td>\n",
       "      <td>AUS</td>\n",
       "      <td>701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Amy Satterthwaite</td>\n",
       "      <td>NZ</td>\n",
       "      <td>661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Chamari Athapaththu</td>\n",
       "      <td>SL</td>\n",
       "      <td>655</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Name Country Rating\n",
       "0         Alyssa Healy      NZ    760\n",
       "1          Beth Mooney     AUS    749\n",
       "2      Laura Wolvaardt      SA    732\n",
       "3       Natalie Sciver     ENG    725\n",
       "4     Harmanpreet Kaur     IND    716\n",
       "5      Smriti Mandhana     IND    714\n",
       "6          Meg Lanning     AUS    710\n",
       "7       Rachael Haynes     AUS    701\n",
       "8    Amy Satterthwaite      NZ    661\n",
       "9  Chamari Athapaththu      SL    655"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# send to get request to the webpage server to get the sorce code of the pages\n",
    "page=requests.get('https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting')\n",
    "\n",
    "#page Content\n",
    "soup=BeautifulSoup(page.content)\n",
    "# print(soup)\n",
    "\n",
    "\n",
    "\n",
    "#name of batsman\n",
    "#firstname of women team\n",
    "fn=soup.find('div',class_='rankings-block__banner--name-large')\n",
    "fn=[str(fn.text)]\n",
    "# print(fn)\n",
    "\n",
    "#fetch all data for name\n",
    "names=[]\n",
    "j=0\n",
    "for i in soup.find_all('td',class_='table-body__cell rankings-table__name name'):\n",
    "    names.append(i.a.text)\n",
    "    j+=1\n",
    "    if (j==9):\n",
    "        break\n",
    "names=fn+names\n",
    "# print(\"Name of batsman = \",names)\n",
    "# print('\\n')\n",
    "\n",
    "#name of country for women team\n",
    "#first name of country\n",
    "fcf=soup.find('div',class_='rankings-block__banner--nationality')\n",
    "fcf=fcf.text.split()\n",
    "# print(fcf)\n",
    "\n",
    "\n",
    "#fetch all data for women team country\n",
    "countrys=[]\n",
    "j=0\n",
    "for i in soup.find_all('span',class_='table-body__logo-text'):\n",
    "    countrys.append(i.text)\n",
    "    j+=1\n",
    "    if (j==9):\n",
    "        break\n",
    "countrys=fc+countrys\n",
    "# print('Country = ',countrys)\n",
    "# print('\\n')\n",
    "\n",
    "\n",
    "# rating of women batsman\n",
    "#first rating of women batsman\n",
    "frw=soup.find('div',class_='rankings-block__banner--rating')\n",
    "frw=[str(frw.text)]\n",
    "# print(frw)\n",
    "\n",
    "ratings=[]\n",
    "j=0\n",
    "for i in soup.find_all('td',class_='table-body__cell rating'):\n",
    "    ratings.append(i.text)\n",
    "    j+=1\n",
    "    if (j==9):\n",
    "        break\n",
    "ratings=fr+ratings\n",
    "# print('Rating = ',ratings)\n",
    "# print('\\n')\n",
    "\n",
    "df=pd.DataFrame({\"Name\":names,\n",
    "                \"Country\":countrys,\n",
    "                \"Rating\":ratings})\n",
    "df"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d38b56dd",
   "metadata": {},
   "source": [
    "c) Top 10 women’s ODI all-rounder along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da6319f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Country</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alyssa Healy</td>\n",
       "      <td>NZ</td>\n",
       "      <td>760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ellyse Perry</td>\n",
       "      <td>AUS</td>\n",
       "      <td>374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Natalie Sciver</td>\n",
       "      <td>ENG</td>\n",
       "      <td>357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Amelia Kerr</td>\n",
       "      <td>NZ</td>\n",
       "      <td>356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Marizanne Kapp</td>\n",
       "      <td>SA</td>\n",
       "      <td>349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Deepti Sharma</td>\n",
       "      <td>IND</td>\n",
       "      <td>322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Ashleigh Gardner</td>\n",
       "      <td>AUS</td>\n",
       "      <td>270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Jess Jonassen</td>\n",
       "      <td>AUS</td>\n",
       "      <td>246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Jhulan Goswami</td>\n",
       "      <td>IND</td>\n",
       "      <td>214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Katherine Brunt</td>\n",
       "      <td>ENG</td>\n",
       "      <td>207</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Name Country Rating\n",
       "0      Alyssa Healy      NZ    760\n",
       "1      Ellyse Perry     AUS    374\n",
       "2    Natalie Sciver     ENG    357\n",
       "3       Amelia Kerr      NZ    356\n",
       "4    Marizanne Kapp      SA    349\n",
       "5     Deepti Sharma     IND    322\n",
       "6  Ashleigh Gardner     AUS    270\n",
       "7     Jess Jonassen     AUS    246\n",
       "8    Jhulan Goswami     IND    214\n",
       "9   Katherine Brunt     ENG    207"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# send to get request to the webpage server to get the sorce code of the pages\n",
    "page=requests.get('https://www.icc-cricket.com/rankings/womens/player-rankings/odi/all-rounder')\n",
    "\n",
    "#page Content\n",
    "soup=BeautifulSoup(page.content)\n",
    "# print(soup)\n",
    "\n",
    "\n",
    "\n",
    "#name of all rounder\n",
    "#firstname\n",
    "fns=soup.find('div',class_='rankings-block__banner--name-large')\n",
    "fns=[str(fns.text)]\n",
    "# print(fns)\n",
    "\n",
    "#fetch all data for name\n",
    "namea=[]\n",
    "j=0\n",
    "for i in soup.find_all('td',class_='table-body__cell rankings-table__name name'):\n",
    "    namea.append(i.a.text)\n",
    "    j+=1\n",
    "    if (j==9):\n",
    "        break\n",
    "name=fn+namea\n",
    "# print(\"Name of all rounder women = \",name)\n",
    "# print('\\n')\n",
    "\n",
    "#name of country for all rounder women\n",
    "#first name of country\n",
    "fca=soup.find('div',class_='rankings-block__banner--nationality')\n",
    "fca=fca.text.split()\n",
    "# print(fca)\n",
    "\n",
    "\n",
    "#fetch all data for country\n",
    "countrya=[]\n",
    "j=0\n",
    "for i in soup.find_all('span',class_='table-body__logo-text'):\n",
    "    countrya.append(i.text)\n",
    "    j+=1\n",
    "    if (j==9):\n",
    "        break\n",
    "country=fc+countrya\n",
    "# print('Country = ',country)\n",
    "# print('\\n')\n",
    "\n",
    "\n",
    "# rating  for all rounder women\n",
    "#first rating\n",
    "fra=soup.find('div',class_='rankings-block__banner--rating')\n",
    "fra=[str(fra.text)]\n",
    "# print(fra)\n",
    "\n",
    "ratinga=[]\n",
    "j=0\n",
    "for i in soup.find_all('td',class_='table-body__cell rating'):\n",
    "    ratinga.append(i.text)\n",
    "    j+=1\n",
    "    if (j==9):\n",
    "        break\n",
    "rating=fr+ratinga\n",
    "# print('Rating = ',rating)\n",
    "# print('\\n')\n",
    "\n",
    "df=pd.DataFrame({\"Name\":name,\n",
    "                \"Country\":country,\n",
    "                \"Rating\":rating})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db58240",
   "metadata": {},
   "source": [
    "# Question 7\n",
    "Write a python program to scrape mentioned news details from https://www.cnbc.com/world/?region=world \n",
    "\n",
    "i) Headline\n",
    "\n",
    "ii) Time\n",
    "\n",
    "iii) News Link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "255acee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "      <th>Time</th>\n",
       "      <th>Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SM Entertainment plans to set up Southeast Asi...</td>\n",
       "      <td>40 Min Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/01/south-korea-sm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>China tech stocks have 'a lot of upside,' fund...</td>\n",
       "      <td>2 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/01/china-internet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'Antiwork' and 'act your wage': Young, disgrun...</td>\n",
       "      <td>4 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/01/young-disgrunt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td>5 Hours Ago</td>\n",
       "      <td>/investingclub/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cramer's lightning round: Crestwood Equity is ...</td>\n",
       "      <td>5 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/11/30/cramers-lightn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td></td>\n",
       "      <td>5 Hours Ago</td>\n",
       "      <td>/investingclub/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Elon Musk meets Tim Cook, says Apple never con...</td>\n",
       "      <td>5 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/11/30/elon-musk-meet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Asia-Pacific markets trade higher as Fed signa...</td>\n",
       "      <td>6 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/01/asia-pacific-s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Jeffrey Epstein estate settles Virgin Islands ...</td>\n",
       "      <td>6 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/11/30/jeffrey-epstei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td></td>\n",
       "      <td>6 Hours Ago</td>\n",
       "      <td>/pro/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td></td>\n",
       "      <td>6 Hours Ago</td>\n",
       "      <td>/pro/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Jim Cramer says to use Wednesday’s rally to re...</td>\n",
       "      <td>6 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/11/30/jim-cramer-use...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>White House weighs future release of emergency...</td>\n",
       "      <td>7 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/11/30/white-house-we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Stock futures tick higher after Wednesday's rally</td>\n",
       "      <td>7 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/11/30/stock-market-f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Stocks making the biggest moves after hours: S...</td>\n",
       "      <td>7 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/11/30/stocks-making-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Jerome Powell says the Fed may ease off 'jumbo...</td>\n",
       "      <td>7 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/11/30/jerome-powell-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>U.S. official says China is a growing threat t...</td>\n",
       "      <td>7 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/11/30/china-is-a-gro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td></td>\n",
       "      <td>7 Hours Ago</td>\n",
       "      <td>/pro/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Broke and down to one credit card: Former FTX ...</td>\n",
       "      <td>7 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/11/30/former-ftx-ceo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td></td>\n",
       "      <td>8 Hours Ago</td>\n",
       "      <td>/investingclub/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td></td>\n",
       "      <td>8 Hours Ago</td>\n",
       "      <td>/pro/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>After Great Resignation and quiet quitting, th...</td>\n",
       "      <td>8 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/11/30/after-great-re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Netflix CEO says he wishes he came around soon...</td>\n",
       "      <td>8 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/11/30/netflix-ceo-re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Snowflake shares drop on light product revenue...</td>\n",
       "      <td>8 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/11/30/snowflake-snow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Salesforce stock falls over 5% on earnings and...</td>\n",
       "      <td>8 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/11/30/salesforce-crm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Bret Taylor steps down as Salesforce co-CEO, l...</td>\n",
       "      <td>8 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/11/30/bret-taylor-st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Mark Zuckerberg says Apple's App Store policie...</td>\n",
       "      <td>9 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/11/30/mark-zuckerber...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>IRS gives Trump tax returns to House committee...</td>\n",
       "      <td>9 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/11/30/irs-gives-trum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>McDonald's is giving away free food for life—h...</td>\n",
       "      <td>9 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/11/30/mcdonalds-is-g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td></td>\n",
       "      <td>9 Hours Ago</td>\n",
       "      <td>/investingclub/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Headline         Time  \\\n",
       "0   SM Entertainment plans to set up Southeast Asi...   40 Min Ago   \n",
       "1   China tech stocks have 'a lot of upside,' fund...  2 Hours Ago   \n",
       "2   'Antiwork' and 'act your wage': Young, disgrun...  4 Hours Ago   \n",
       "3                                                      5 Hours Ago   \n",
       "4   Cramer's lightning round: Crestwood Equity is ...  5 Hours Ago   \n",
       "5                                                      5 Hours Ago   \n",
       "6   Elon Musk meets Tim Cook, says Apple never con...  5 Hours Ago   \n",
       "7   Asia-Pacific markets trade higher as Fed signa...  6 Hours Ago   \n",
       "8   Jeffrey Epstein estate settles Virgin Islands ...  6 Hours Ago   \n",
       "9                                                      6 Hours Ago   \n",
       "10                                                     6 Hours Ago   \n",
       "11  Jim Cramer says to use Wednesday’s rally to re...  6 Hours Ago   \n",
       "12  White House weighs future release of emergency...  7 Hours Ago   \n",
       "13  Stock futures tick higher after Wednesday's rally  7 Hours Ago   \n",
       "14  Stocks making the biggest moves after hours: S...  7 Hours Ago   \n",
       "15  Jerome Powell says the Fed may ease off 'jumbo...  7 Hours Ago   \n",
       "16  U.S. official says China is a growing threat t...  7 Hours Ago   \n",
       "17                                                     7 Hours Ago   \n",
       "18  Broke and down to one credit card: Former FTX ...  7 Hours Ago   \n",
       "19                                                     8 Hours Ago   \n",
       "20                                                     8 Hours Ago   \n",
       "21  After Great Resignation and quiet quitting, th...  8 Hours Ago   \n",
       "22  Netflix CEO says he wishes he came around soon...  8 Hours Ago   \n",
       "23  Snowflake shares drop on light product revenue...  8 Hours Ago   \n",
       "24  Salesforce stock falls over 5% on earnings and...  8 Hours Ago   \n",
       "25  Bret Taylor steps down as Salesforce co-CEO, l...  8 Hours Ago   \n",
       "26  Mark Zuckerberg says Apple's App Store policie...  9 Hours Ago   \n",
       "27  IRS gives Trump tax returns to House committee...  9 Hours Ago   \n",
       "28  McDonald's is giving away free food for life—h...  9 Hours Ago   \n",
       "29                                                     9 Hours Ago   \n",
       "\n",
       "                                                 Link  \n",
       "0   https://www.cnbc.com/2022/12/01/south-korea-sm...  \n",
       "1   https://www.cnbc.com/2022/12/01/china-internet...  \n",
       "2   https://www.cnbc.com/2022/12/01/young-disgrunt...  \n",
       "3                                     /investingclub/  \n",
       "4   https://www.cnbc.com/2022/11/30/cramers-lightn...  \n",
       "5                                     /investingclub/  \n",
       "6   https://www.cnbc.com/2022/11/30/elon-musk-meet...  \n",
       "7   https://www.cnbc.com/2022/12/01/asia-pacific-s...  \n",
       "8   https://www.cnbc.com/2022/11/30/jeffrey-epstei...  \n",
       "9                                               /pro/  \n",
       "10                                              /pro/  \n",
       "11  https://www.cnbc.com/2022/11/30/jim-cramer-use...  \n",
       "12  https://www.cnbc.com/2022/11/30/white-house-we...  \n",
       "13  https://www.cnbc.com/2022/11/30/stock-market-f...  \n",
       "14  https://www.cnbc.com/2022/11/30/stocks-making-...  \n",
       "15  https://www.cnbc.com/2022/11/30/jerome-powell-...  \n",
       "16  https://www.cnbc.com/2022/11/30/china-is-a-gro...  \n",
       "17                                              /pro/  \n",
       "18  https://www.cnbc.com/2022/11/30/former-ftx-ceo...  \n",
       "19                                    /investingclub/  \n",
       "20                                              /pro/  \n",
       "21  https://www.cnbc.com/2022/11/30/after-great-re...  \n",
       "22  https://www.cnbc.com/2022/11/30/netflix-ceo-re...  \n",
       "23  https://www.cnbc.com/2022/11/30/snowflake-snow...  \n",
       "24  https://www.cnbc.com/2022/11/30/salesforce-crm...  \n",
       "25  https://www.cnbc.com/2022/11/30/bret-taylor-st...  \n",
       "26  https://www.cnbc.com/2022/11/30/mark-zuckerber...  \n",
       "27  https://www.cnbc.com/2022/11/30/irs-gives-trum...  \n",
       "28  https://www.cnbc.com/2022/11/30/mcdonalds-is-g...  \n",
       "29                                    /investingclub/  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# send to get request to the webpage server to get the sorce code of the page\n",
    "page=requests.get('https://www.cnbc.com/world/?region=world')\n",
    "#print(page)\n",
    "soup=BeautifulSoup(page.content)\n",
    "#print(soup)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#for find first Headline of news\n",
    "Headline=soup.find('div',class_='LatestNews-headlineWrapper')\n",
    "# print(Headline.a.text)\n",
    "\n",
    "#for find first time of news\n",
    "Time=soup.find('div',class_='LatestNews-headlineWrapper')\n",
    "# print(Time.span.text)\n",
    "\n",
    "#for find first link of news\n",
    "Link=soup.find('div',class_='LatestNews-headlineWrapper')\n",
    "# print(Link.a.get('href'))\n",
    "\n",
    "\n",
    "#for find all Headlines,Times and links\n",
    "headline=[]\n",
    "link=[]\n",
    "time=[]\n",
    "for i in soup.find_all('div',class_='LatestNews-headlineWrapper'):\n",
    "    k=i.a.text\n",
    "    j=i.span.text\n",
    "    l=i.a.get(\"href\")\n",
    "    headline.append(k)\n",
    "    link.append(l)\n",
    "    time.append(j)\n",
    "# print('Headline = ',headline)\n",
    "# print('\\n')\n",
    "# print(\"Link = \",link)\n",
    "# print('\\n')\n",
    "# print(\"Time = \",time)\n",
    "# print('\\n')\n",
    "\n",
    "#creation of DataFrame\n",
    "df=pd.DataFrame({\n",
    "    'Headline':headline,\n",
    "    'Time':time,\n",
    "    'Link':link\n",
    "})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0525b4cf",
   "metadata": {},
   "source": [
    "# Question 8\n",
    "Write a python program to scrape the details of most downloaded articles from AI in last 90 days.\n",
    "https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\n",
    "\n",
    "Scrape below mentioned details :\n",
    "\n",
    "i) Paper Title\n",
    "\n",
    "ii) Authors\n",
    "\n",
    "iii) Published Date\n",
    "\n",
    "iv) Paper URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cadff663",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Paper Title</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Published Date</th>\n",
       "      <th>Paper URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Reward is enough</td>\n",
       "      <td>Silver, David, Singh, Satinder, Precup, Doina,...</td>\n",
       "      <td>October 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Making sense of raw input</td>\n",
       "      <td>Evans, Richard, Bošnjak, Matko and 5 more</td>\n",
       "      <td>October 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Law and logic: A review from an argumentation ...</td>\n",
       "      <td>Prakken, Henry, Sartor, Giovanni</td>\n",
       "      <td>October 2015</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Creativity and artificial intelligence</td>\n",
       "      <td>Boden, Margaret A.</td>\n",
       "      <td>August 1998</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Artificial cognition for social human–robot in...</td>\n",
       "      <td>Lemaignan, Séverin, Warnier, Mathieu and 3 more</td>\n",
       "      <td>June 2017</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Explanation in artificial intelligence: Insigh...</td>\n",
       "      <td>Miller, Tim</td>\n",
       "      <td>February 2019</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Making sense of sensory input</td>\n",
       "      <td>Evans, Richard, Hernández-Orallo, José and 3 more</td>\n",
       "      <td>April 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Conflict-based search for optimal multi-agent ...</td>\n",
       "      <td>Sharon, Guni, Stern, Roni, Felner, Ariel, Stur...</td>\n",
       "      <td>February 2015</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Between MDPs and semi-MDPs: A framework for te...</td>\n",
       "      <td>Sutton, Richard S., Precup, Doina, Singh, Sati...</td>\n",
       "      <td>August 1999</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The Hanabi challenge: A new frontier for AI re...</td>\n",
       "      <td>Bard, Nolan, Foerster, Jakob N. and 13 more</td>\n",
       "      <td>March 2020</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Evaluating XAI: A comparison of rule-based and...</td>\n",
       "      <td>van der Waa, Jasper, Nieuwburg, Elisabeth, Cre...</td>\n",
       "      <td>February 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Argumentation in artificial intelligence</td>\n",
       "      <td>Bench-Capon, T.J.M., Dunne, Paul E.</td>\n",
       "      <td>October 2007</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Algorithms for computing strategies in two-pla...</td>\n",
       "      <td>Bošanský, Branislav, Lisý, Viliam and 3 more</td>\n",
       "      <td>August 2016</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Multiple object tracking: A literature review</td>\n",
       "      <td>Luo, Wenhan, Xing, Junliang and 4 more</td>\n",
       "      <td>April 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Selection of relevant features and examples in...</td>\n",
       "      <td>Blum, Avrim L., Langley, Pat</td>\n",
       "      <td>December 1997</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>A survey of inverse reinforcement learning: Ch...</td>\n",
       "      <td>Arora, Saurabh, Doshi, Prashant</td>\n",
       "      <td>August 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Explaining individual predictions when feature...</td>\n",
       "      <td>Aas, Kjersti, Jullum, Martin, Løland, Anders</td>\n",
       "      <td>September 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>A review of possible effects of cognitive bias...</td>\n",
       "      <td>Kliegr, Tomáš, Bahník, Štěpán, Fürnkranz, Joha...</td>\n",
       "      <td>June 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Integrating social power into the decision-mak...</td>\n",
       "      <td>Pereira, Gonçalo, Prada, Rui, Santos, Pedro A.</td>\n",
       "      <td>December 2016</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>“That's (not) the output I expected!” On the r...</td>\n",
       "      <td>Riveiro, Maria, Thill, Serge</td>\n",
       "      <td>September 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Explaining black-box classifiers using post-ho...</td>\n",
       "      <td>Kenny, Eoin M., Ford, Courtney, Quinn, Molly, ...</td>\n",
       "      <td>May 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Algorithm runtime prediction: Methods &amp; evalua...</td>\n",
       "      <td>Hutter, Frank, Xu, Lin, Hoos, Holger H., Leyto...</td>\n",
       "      <td>January 2014</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Wrappers for feature subset selection</td>\n",
       "      <td>Kohavi, Ron, John, George H.</td>\n",
       "      <td>December 1997</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Commonsense visual sensemaking for autonomous ...</td>\n",
       "      <td>Suchan, Jakob, Bhatt, Mehul, Varadarajan, Srik...</td>\n",
       "      <td>October 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Quantum computation, quantum theory and AI</td>\n",
       "      <td>Ying, Mingsheng</td>\n",
       "      <td>February 2010</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Paper Title  \\\n",
       "0                                    Reward is enough   \n",
       "1                           Making sense of raw input   \n",
       "2   Law and logic: A review from an argumentation ...   \n",
       "3              Creativity and artificial intelligence   \n",
       "4   Artificial cognition for social human–robot in...   \n",
       "5   Explanation in artificial intelligence: Insigh...   \n",
       "6                       Making sense of sensory input   \n",
       "7   Conflict-based search for optimal multi-agent ...   \n",
       "8   Between MDPs and semi-MDPs: A framework for te...   \n",
       "9   The Hanabi challenge: A new frontier for AI re...   \n",
       "10  Evaluating XAI: A comparison of rule-based and...   \n",
       "11           Argumentation in artificial intelligence   \n",
       "12  Algorithms for computing strategies in two-pla...   \n",
       "13      Multiple object tracking: A literature review   \n",
       "14  Selection of relevant features and examples in...   \n",
       "15  A survey of inverse reinforcement learning: Ch...   \n",
       "16  Explaining individual predictions when feature...   \n",
       "17  A review of possible effects of cognitive bias...   \n",
       "18  Integrating social power into the decision-mak...   \n",
       "19  “That's (not) the output I expected!” On the r...   \n",
       "20  Explaining black-box classifiers using post-ho...   \n",
       "21  Algorithm runtime prediction: Methods & evalua...   \n",
       "22              Wrappers for feature subset selection   \n",
       "23  Commonsense visual sensemaking for autonomous ...   \n",
       "24         Quantum computation, quantum theory and AI   \n",
       "\n",
       "                                              Authors  Published Date  \\\n",
       "0   Silver, David, Singh, Satinder, Precup, Doina,...    October 2021   \n",
       "1           Evans, Richard, Bošnjak, Matko and 5 more    October 2021   \n",
       "2                   Prakken, Henry, Sartor, Giovanni     October 2015   \n",
       "3                                 Boden, Margaret A.      August 1998   \n",
       "4     Lemaignan, Séverin, Warnier, Mathieu and 3 more       June 2017   \n",
       "5                                        Miller, Tim    February 2019   \n",
       "6   Evans, Richard, Hernández-Orallo, José and 3 more      April 2021   \n",
       "7   Sharon, Guni, Stern, Roni, Felner, Ariel, Stur...   February 2015   \n",
       "8   Sutton, Richard S., Precup, Doina, Singh, Sati...     August 1999   \n",
       "9         Bard, Nolan, Foerster, Jakob N. and 13 more      March 2020   \n",
       "10  van der Waa, Jasper, Nieuwburg, Elisabeth, Cre...   February 2021   \n",
       "11               Bench-Capon, T.J.M., Dunne, Paul E.     October 2007   \n",
       "12       Bošanský, Branislav, Lisý, Viliam and 3 more     August 2016   \n",
       "13             Luo, Wenhan, Xing, Junliang and 4 more      April 2021   \n",
       "14                      Blum, Avrim L., Langley, Pat    December 1997   \n",
       "15                   Arora, Saurabh, Doshi, Prashant      August 2021   \n",
       "16      Aas, Kjersti, Jullum, Martin, Løland, Anders   September 2021   \n",
       "17  Kliegr, Tomáš, Bahník, Štěpán, Fürnkranz, Joha...       June 2021   \n",
       "18    Pereira, Gonçalo, Prada, Rui, Santos, Pedro A.    December 2016   \n",
       "19                      Riveiro, Maria, Thill, Serge   September 2021   \n",
       "20  Kenny, Eoin M., Ford, Courtney, Quinn, Molly, ...        May 2021   \n",
       "21  Hutter, Frank, Xu, Lin, Hoos, Holger H., Leyto...    January 2014   \n",
       "22                      Kohavi, Ron, John, George H.    December 1997   \n",
       "23  Suchan, Jakob, Bhatt, Mehul, Varadarajan, Srik...    October 2021   \n",
       "24                                   Ying, Mingsheng    February 2010   \n",
       "\n",
       "                                            Paper URL  \n",
       "0   https://www.sciencedirect.com/science/article/...  \n",
       "1   https://www.sciencedirect.com/science/article/...  \n",
       "2   https://www.sciencedirect.com/science/article/...  \n",
       "3   https://www.sciencedirect.com/science/article/...  \n",
       "4   https://www.sciencedirect.com/science/article/...  \n",
       "5   https://www.sciencedirect.com/science/article/...  \n",
       "6   https://www.sciencedirect.com/science/article/...  \n",
       "7   https://www.sciencedirect.com/science/article/...  \n",
       "8   https://www.sciencedirect.com/science/article/...  \n",
       "9   https://www.sciencedirect.com/science/article/...  \n",
       "10  https://www.sciencedirect.com/science/article/...  \n",
       "11  https://www.sciencedirect.com/science/article/...  \n",
       "12  https://www.sciencedirect.com/science/article/...  \n",
       "13  https://www.sciencedirect.com/science/article/...  \n",
       "14  https://www.sciencedirect.com/science/article/...  \n",
       "15  https://www.sciencedirect.com/science/article/...  \n",
       "16  https://www.sciencedirect.com/science/article/...  \n",
       "17  https://www.sciencedirect.com/science/article/...  \n",
       "18  https://www.sciencedirect.com/science/article/...  \n",
       "19  https://www.sciencedirect.com/science/article/...  \n",
       "20  https://www.sciencedirect.com/science/article/...  \n",
       "21  https://www.sciencedirect.com/science/article/...  \n",
       "22  https://www.sciencedirect.com/science/article/...  \n",
       "23  https://www.sciencedirect.com/science/article/...  \n",
       "24  https://www.sciencedirect.com/science/article/...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# send to get request to the webpage server to get the sorce code of the page\n",
    "page=requests.get('https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles')\n",
    "# print(page)\n",
    "\n",
    "#page Content\n",
    "soup=BeautifulSoup(page.content)\n",
    "# print(soup)\n",
    "\n",
    "\n",
    "\n",
    "#for paper Title and link of paper\n",
    "title1=soup.find('a',class_='sc-5smygv-0 fIXTHm')\n",
    "# print(title1.get('href'))\n",
    "# print(title1.text)\n",
    "\n",
    "# for all paper Title and links\n",
    "link=[]\n",
    "title=[]\n",
    "for i in soup.find_all('a',class_='sc-5smygv-0 fIXTHm'):\n",
    "    title.append(i.text)\n",
    "    link.append(i.get('href'))\n",
    "\n",
    "# print('Link of paper = ',link)\n",
    "# print('\\n')\n",
    "# print('Title of paper = ',title)\n",
    "# print('\\n')\n",
    "\n",
    "\n",
    "\n",
    "# paper Author\n",
    "Author=soup.find('span',class_='sc-1w3fpd7-0 dnCnAO')\n",
    "# print(Author.text)\n",
    "\n",
    "#find all authors \n",
    "Authors=[]\n",
    "for i in soup.find_all('span',class_='sc-1w3fpd7-0 dnCnAO'):\n",
    "    Authors.append(i.text)\n",
    "# print('Authors = ',Authors)\n",
    "\n",
    "\n",
    "#paper Published Date\n",
    "Published=soup.find('span',class_='sc-1thf9ly-2 dvggWt')\n",
    "# print(Published.text)\n",
    "\n",
    "#find all Published Date \n",
    "Published_Date=[]\n",
    "for i in soup.find_all('span',class_='sc-1thf9ly-2 dvggWt'):\n",
    "    Published_Date.append(i.text)\n",
    "\n",
    "# print('Published Date = ',Published_Date)\n",
    "\n",
    "#creation of DataFrame\n",
    "df=pd.DataFrame({\n",
    "    'Paper Title':title,\n",
    "    'Authors':Authors,\n",
    "    'Published Date':Published_Date,\n",
    "    'Paper URL':link\n",
    "})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97a34b2",
   "metadata": {},
   "source": [
    "# Question 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de52f11",
   "metadata": {},
   "source": [
    "Write a python program to scrape mentioned details from dineout.co.in :\n",
    "i) Restaurant name\n",
    "\n",
    "ii) Cuisine\n",
    "\n",
    "iii) Location\n",
    "\n",
    "iv) Ratings\n",
    "\n",
    "v) Image URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "732c175c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Restaurant name</th>\n",
       "      <th>Cuisine</th>\n",
       "      <th>Location</th>\n",
       "      <th>Ratings</th>\n",
       "      <th>Price</th>\n",
       "      <th>Images</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Castle Barbeque</td>\n",
       "      <td>Chinese, North Indian</td>\n",
       "      <td>Connaught Place, Central Delhi</td>\n",
       "      <td>4.1</td>\n",
       "      <td>₹ 2,000 for 2 (approx)</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jungle Jamboree</td>\n",
       "      <td>North Indian, Asian, Italian</td>\n",
       "      <td>3CS Mall,Lajpat Nagar - 3, South Delhi</td>\n",
       "      <td>3.9</td>\n",
       "      <td>₹ 1,680 for 2 (approx)</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cafe Knosh</td>\n",
       "      <td>Italian, Continental</td>\n",
       "      <td>The Leela Ambience Convention Hotel,Shahdara, ...</td>\n",
       "      <td>4.3</td>\n",
       "      <td>₹ 3,000 for 2 (approx)</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Castle Barbeque</td>\n",
       "      <td>Chinese, North Indian</td>\n",
       "      <td>Pacific Mall,Tagore Garden, West Delhi</td>\n",
       "      <td>3.9</td>\n",
       "      <td>₹ 2,000 for 2 (approx)</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Barbeque Company</td>\n",
       "      <td>North Indian, Chinese</td>\n",
       "      <td>Gardens Galleria,Sector 38A, Noida</td>\n",
       "      <td>4</td>\n",
       "      <td>₹ 1,700 for 2 (approx)</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>India Grill</td>\n",
       "      <td>North Indian, Italian</td>\n",
       "      <td>Hilton Garden Inn,Saket, South Delhi</td>\n",
       "      <td>3.9</td>\n",
       "      <td>₹ 2,400 for 2 (approx)</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Delhi Barbeque</td>\n",
       "      <td>North Indian</td>\n",
       "      <td>Taurus Sarovar Portico,Mahipalpur, South Delhi</td>\n",
       "      <td>3.6</td>\n",
       "      <td>₹ 1,800 for 2 (approx)</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The Monarch - Bar Be Que Village</td>\n",
       "      <td>North Indian</td>\n",
       "      <td>Indirapuram Habitat Centre,Indirapuram, Ghaziabad</td>\n",
       "      <td>3.8</td>\n",
       "      <td>₹ 1,900 for 2 (approx)</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Indian Grill Room</td>\n",
       "      <td>North Indian, Mughlai</td>\n",
       "      <td>Suncity Business Tower,Golf Course Road, Gurgaon</td>\n",
       "      <td>4.3</td>\n",
       "      <td>₹ 2,200 for 2 (approx)</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Restaurant name                        Cuisine  \\\n",
       "0                   Castle Barbeque          Chinese, North Indian   \n",
       "1                   Jungle Jamboree   North Indian, Asian, Italian   \n",
       "2                        Cafe Knosh           Italian, Continental   \n",
       "3                   Castle Barbeque          Chinese, North Indian   \n",
       "4              The Barbeque Company          North Indian, Chinese   \n",
       "5                       India Grill          North Indian, Italian   \n",
       "6                    Delhi Barbeque                   North Indian   \n",
       "7  The Monarch - Bar Be Que Village                   North Indian   \n",
       "8                 Indian Grill Room          North Indian, Mughlai   \n",
       "\n",
       "                                            Location Ratings  \\\n",
       "0                     Connaught Place, Central Delhi     4.1   \n",
       "1             3CS Mall,Lajpat Nagar - 3, South Delhi     3.9   \n",
       "2  The Leela Ambience Convention Hotel,Shahdara, ...     4.3   \n",
       "3             Pacific Mall,Tagore Garden, West Delhi     3.9   \n",
       "4                 Gardens Galleria,Sector 38A, Noida       4   \n",
       "5               Hilton Garden Inn,Saket, South Delhi     3.9   \n",
       "6     Taurus Sarovar Portico,Mahipalpur, South Delhi     3.6   \n",
       "7  Indirapuram Habitat Centre,Indirapuram, Ghaziabad     3.8   \n",
       "8   Suncity Business Tower,Golf Course Road, Gurgaon     4.3   \n",
       "\n",
       "                     Price                                             Images  \n",
       "0  ₹ 2,000 for 2 (approx)   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "1  ₹ 1,680 for 2 (approx)   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "2  ₹ 3,000 for 2 (approx)   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "3  ₹ 2,000 for 2 (approx)   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "4  ₹ 1,700 for 2 (approx)   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "5  ₹ 2,400 for 2 (approx)   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "6  ₹ 1,800 for 2 (approx)   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "7  ₹ 1,900 for 2 (approx)   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "8  ₹ 2,200 for 2 (approx)   https://im1.dineout.co.in/images/uploads/resta...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# send to get request to the webpage server to get the sorce code of the page\n",
    "page=requests.get(\"https://www.dineout.co.in/delhi-restaurants/buffet-special\")\n",
    "# print(page)\n",
    "\n",
    "#page Content\n",
    "soup =BeautifulSoup(page.content)\n",
    "# print(soup)\n",
    "\n",
    "\n",
    "\n",
    "#Scraping first Name\n",
    "first_title=soup.find('a',class_=\"restnt-name ellipsis\")\n",
    "# print(first_title)\n",
    "# print(\"first_title=\",first_title.text)\n",
    "# print('\\n')\n",
    "\n",
    "#Scraping Multiple Titles\n",
    "title=[]\n",
    "for i in soup.find_all('a',class_=\"restnt-name ellipsis\"):\n",
    "    title.append(i.text)\n",
    "    \n",
    "# print(\"titles=\",title)\n",
    "# print('\\n')\n",
    "\n",
    "\n",
    "\n",
    "#Scraping first Location\n",
    "loc=soup.find('div',class_='restnt-loc ellipsis')\n",
    "# print(loc)\n",
    "# print('Location=',loc.text)\n",
    "# print('\\n')\n",
    "\n",
    "#Scraping Multiple Location\n",
    "Location=[]\n",
    "for i in soup.find_all('div',class_='restnt-loc ellipsis'):\n",
    "    Location.append(i.text)\n",
    "    \n",
    "# print(\"Location=\",Location)\n",
    "# print('\\n')\n",
    "\n",
    "# Scraping first price\n",
    "price=soup.find('span',class_='double-line-ellipsis')\n",
    "# print(price)\n",
    "# print(price.text)\n",
    "# print(price.text.split('|'))\n",
    "# print(price.text.split('|')[0])\n",
    "# print('\\n')\n",
    "\n",
    "#Scraping Multiple Price\n",
    "Price=[]\n",
    "for i in soup.find_all('span',class_='double-line-ellipsis'):\n",
    "    Price.append(i.text.split('|')[0])\n",
    "    \n",
    "# print(\"Price=\",Price)\n",
    "# print('\\n')\n",
    "\n",
    "#Scraping Multiple images\n",
    "images=[]\n",
    "for i in soup.find_all('img',class_='no-img'):\n",
    "    images.append(i.get('data-src'))\n",
    "    \n",
    "# print(\"Images=\",images)\n",
    "# print('\\n')\n",
    "\n",
    "# Scraping first Cuisine\n",
    "Cuisine=soup.find('span',class_='double-line-ellipsis')\n",
    "# print(Cuisine)\n",
    "# print(Cuisine.text)\n",
    "# print(Cuisine.text.split('|'))\n",
    "# print(Cuisine.text.split('|')[1])\n",
    "# print('\\n')\n",
    "\n",
    "#Scraping Multiple Cuisine\n",
    "Cuisine=[]\n",
    "for i in soup.find_all('span',class_='double-line-ellipsis'):\n",
    "    Cuisine.append(i.text.split('|')[1])\n",
    "    \n",
    "# print(\"Cuisine=\",Cuisine)\n",
    "# print('\\n')\n",
    "\n",
    "\n",
    "# Scraping first Rating\n",
    "Rating=soup.find('div',class_='restnt-rating rating-4')\n",
    "# print(Rating)\n",
    "# print(Rating.text)\n",
    "# print('\\n')\n",
    "\n",
    "#Scraping Multiple Rating\n",
    "Rating=[]\n",
    "for i in soup.find_all('div',class_='restnt-rating rating-4'):\n",
    "    Rating.append(i.text)\n",
    "    \n",
    "# print(\"Rating=\",Rating)\n",
    "# print('\\n')\n",
    "\n",
    "\n",
    "# create Dataframe\n",
    "df=pd.DataFrame({'Restaurant name':title,\n",
    "                 'Cuisine':Cuisine,\n",
    "                'Location':Location,\n",
    "                 'Ratings':Rating,\n",
    "                'Price':Price,\n",
    "                'Images':images})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f165be96",
   "metadata": {},
   "source": [
    "# Question 10\n",
    "Write a python program to scrape the details of top publications from Google Scholar from\n",
    "https://scholar.google.com/citations?view_op=top_venues&hl=en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c15143eb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank of publications</th>\n",
       "      <th>Name of publications</th>\n",
       "      <th>Publication h5-indexs</th>\n",
       "      <th>Publication h5-median</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Nature</td>\n",
       "      <td>444</td>\n",
       "      <td>667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>The New England Journal of Medicine</td>\n",
       "      <td>432</td>\n",
       "      <td>780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Science</td>\n",
       "      <td>401</td>\n",
       "      <td>614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>IEEE/CVF Conference on Computer Vision and Pat...</td>\n",
       "      <td>389</td>\n",
       "      <td>627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>The Lancet</td>\n",
       "      <td>354</td>\n",
       "      <td>635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>96</td>\n",
       "      <td>Journal of Business Research</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97</td>\n",
       "      <td>Molecular Cancer</td>\n",
       "      <td>145</td>\n",
       "      <td>209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98</td>\n",
       "      <td>Sensors</td>\n",
       "      <td>145</td>\n",
       "      <td>201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99</td>\n",
       "      <td>Nature Climate Change</td>\n",
       "      <td>144</td>\n",
       "      <td>228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>100</td>\n",
       "      <td>IEEE Internet of Things Journal</td>\n",
       "      <td>144</td>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Rank of publications                               Name of publications  \\\n",
       "0                      1                                             Nature   \n",
       "1                      2                The New England Journal of Medicine   \n",
       "2                      3                                            Science   \n",
       "3                      4  IEEE/CVF Conference on Computer Vision and Pat...   \n",
       "4                      5                                         The Lancet   \n",
       "..                   ...                                                ...   \n",
       "95                    96                       Journal of Business Research   \n",
       "96                    97                                   Molecular Cancer   \n",
       "97                    98                                            Sensors   \n",
       "98                    99                              Nature Climate Change   \n",
       "99                   100                    IEEE Internet of Things Journal   \n",
       "\n",
       "   Publication h5-indexs Publication h5-median  \n",
       "0                    444                   667  \n",
       "1                    432                   780  \n",
       "2                    401                   614  \n",
       "3                    389                   627  \n",
       "4                    354                   635  \n",
       "..                   ...                   ...  \n",
       "95                   145                   233  \n",
       "96                   145                   209  \n",
       "97                   145                   201  \n",
       "98                   144                   228  \n",
       "99                   144                   212  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# send to get request to the webpage server to get the sorce code of the page\n",
    "page=requests.get(\"https://scholar.google.com/citations?view_op=top_venues&hl=en\")\n",
    "# print(page)\n",
    "\n",
    "#page Content\n",
    "soup =BeautifulSoup(page.content)\n",
    "# print(soup)\n",
    "\n",
    "\n",
    "#find rank of publications\n",
    "rank=soup.find('td',class_='gsc_mvt_p')\n",
    "# print(rank.text)\n",
    "#Scraping multiple ranks\n",
    "rank=[]\n",
    "for i in soup.find_all('td',class_='gsc_mvt_p'):\n",
    "    i=i.text\n",
    "    i=int(re.sub(r'[^0-9]','',i))\n",
    "    rank.append(i)\n",
    "# print('Rank of publications = ',rank)\n",
    "# print('\\n')\n",
    "\n",
    "\n",
    "\n",
    "#find name of publications\n",
    "publications=soup.find('td',class_='gsc_mvt_t')\n",
    "# print(publications.text)\n",
    "#scraping multiple publications\n",
    "publications=[]\n",
    "for i in soup.find_all('td',class_='gsc_mvt_t'):\n",
    "    publications.append(i.text)\n",
    "# print('Name of publications = ',publications)\n",
    "# print('\\n')\n",
    "\n",
    "\n",
    "\n",
    "#find h5-index,h5-median\n",
    "#find h5-index\n",
    "h5_index=soup.find('td',class_='gsc_mvt_n')\n",
    "# print(h5_index.text)\n",
    "#find h5-median\n",
    "h5_median=soup.find('span',class_='gs_ibl gsc_mp_anchor')\n",
    "# print(h5_median.text)\n",
    "\n",
    "#Scraping multiple h5-indexs & h5-median\n",
    "h5_index_median=[]\n",
    "for i in soup.find_all('td',class_='gsc_mvt_n'):\n",
    "    h5_index_median.append(i.text)\n",
    "# print(\"Publication h5-indexs && h5-median = \",h5_index_median)\n",
    "# print('\\n')\n",
    "\n",
    "\n",
    "h5_index=[]\n",
    "h5_median=[]\n",
    "x=len(h5_index_median)\n",
    "for i in range(0,x):\n",
    "    if(i%2==0):\n",
    "        h5_index.append(h5_index_median[i])\n",
    "    else:\n",
    "        h5_median.append(h5_index_median[i])\n",
    "\n",
    "# print(\"Publication h5-index = \",h5_index)\n",
    "# print('\\n')\n",
    "# print(\"Publication h5-median = \",h5_median)\n",
    "# print('\\n')\n",
    "\n",
    "#creation of dataframe\n",
    "df=pd.DataFrame({'Rank of publications':rank,\n",
    "                'Name of publications':publications,\n",
    "                \"Publication h5-indexs\":h5_index,\n",
    "                \"Publication h5-median\":h5_median})\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
